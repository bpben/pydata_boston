{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyData NLP workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'spacy-transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This section shows some of the considerations to make when tokenizing your data.\n",
    "\n",
    "Token = \"Useful semantic unit\"\n",
    "\n",
    "But what does that mean? This section will detail some considerations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize in Chinese: ['我们', '正在', '做', 'NLP', '。']\n",
      "Tokenize in English: ['我们正在做NLP', '。']\n"
     ]
    }
   ],
   "source": [
    "# importing different languages in spacy\n",
    "# blank English model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "# blank Chinese model\n",
    "# to run, will need to install jieba tokenizer (optional)\n",
    "#!pip install jieba\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "zh = spacy.lang.zh.Chinese()\n",
    "zh_text = '我们正在做NLP。'\n",
    "print('Tokenize in Chinese:', [x.text for x in zh(zh_text)])\n",
    "print('Tokenize in English:', [x.text for x in en(zh_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base python:  we are doing nlp.\n",
      "SpaCy: ['we', 'are', 'doing', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "# lowercasing\n",
    "text = 'We are doing NLP.'\n",
    "print('Base python: ', text.lower())\n",
    "print('SpaCy:', [x.lower_ for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are doing NLP\n",
      "['We', 'are', 'doing', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# handling non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print([x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just removing punctuation: Were doing NLP\n",
      "Removing non-alpha ['We', 'doing', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# but what about contractions?\n",
    "text = \"We're doing NLP.\"\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print('Just removing punctuation:', re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print('Removing non-alpha', [x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the is_alpha flag is False for any tokens that have non-alpha characters.  We'll look into a better way for dealing with contractions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a tokenizer\n",
    "In this exercise, you will make a function that uses spaCy's base English model to tokenize a dataset according to specific parameters.  The functions will take a list of documents and output a list of tokens.  In this case we're interested in outputting strings, rather than spaCy tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "en = English()\n",
    "\n",
    "def tokenize_base(docs, model=en):\n",
    "    # tokenizer that just parses using spaCy's base model\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.text for t in parsed])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha(docs, model=en):\n",
    "    # tokenizer that lowercases and removes any non-alpha character\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if t.is_alpha])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha_url(docs, model=en):\n",
    "    # tokenizer that lowercases, removes any non-alpha character and removes urls\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "Though word tense can sometimes carry with it a lot of useful information, a lot of time it might be useful to reduce words to their common root.  For example, the word \"be\" has various forms like \"are\", \"is\", \"been\".  We might not want our vocabulary to contain all these forms and rather count them all as instances of \"be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am taking an NLP course.\n",
      "['-PRON-', 'be', 'take', 'an', 'NLP', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course.'\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Dealing with stop words involves making some pretty impactful decisions with your data.  Refer to the slides for details.  Here, we just remove stop words based on [spaCy's default set](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In June 2020, I took a course at Harvard Extension School in Cambridge.\n",
      "['June', '2020', ',', 'took', 'course', 'Harvard', 'Extension', 'School', 'Cambridge', '.']\n"
     ]
    }
   ],
   "source": [
    "en = English()\n",
    "text = 'In June 2020, I took a course at Harvard Extension School in Cambridge.'\n",
    "print(text)\n",
    "print([x.text for x in en(text) if not x.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-standard tokens (e.g. named-entities)\n",
    "In text, some some n-grams should not be treated as a concatenation of unigrams.  For example, New York City is fundamentally different from the individual words \"new\", \"york\" and \"city\".\n",
    "\n",
    "Here we attempt to deal with some of these non-standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out these courses: https://www.summer.harvard.edu/\n",
      "Check out these courses: \n",
      "[Check, out, these, courses, :]\n",
      "[Check, out, these, courses, :, '-URL-']\n"
     ]
    }
   ],
   "source": [
    "# urls\n",
    "# base python\n",
    "# regex from textacy: https://github.com/chartbeat-labs/textacy\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out these courses: https://www.summer.harvard.edu/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))\n",
    "# spacy\n",
    "print([x for x in en(text) if not x.like_url])\n",
    "# spacy - replace with a standard token\n",
    "print(['-URL-' if x.like_url else x for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, am, taking, an, NLP, course, at, Harvard, starting, July, 19th, ,, 2020]\n",
      "NLP <class 'spacy.tokens.span.Span'> ORG Companies, agencies, institutions, etc.\n",
      "Harvard <class 'spacy.tokens.span.Span'> ORG Companies, agencies, institutions, etc.\n",
      "July 19th, 2020 <class 'spacy.tokens.span.Span'> DATE Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "# named-entities\n",
    "# read in English model with tagging/entity pipeline components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course at Harvard starting July 19th, 2020'\n",
    "parsed = nlp(text)\n",
    "# look at the individual tokens\n",
    "tokens = [t for t in parsed]\n",
    "print(tokens)\n",
    "# look at the identified named-entities and their types\n",
    "for e in parsed.ents:\n",
    "    print(e, type(e), e.label_, spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A comprehensive tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-PRON-', 'be', 'take', 'a', 'course', 'at', 'Harvard', '.'],\n",
       " ['-PRON-', 'be', 'learn', 'about', 'Natural', 'Language', 'Processing', '.'],\n",
       " ['-PRON-',\n",
       "  'be',\n",
       "  'study',\n",
       "  'tokenization',\n",
       "  ',',\n",
       "  'vectorization',\n",
       "  'and',\n",
       "  'modelling',\n",
       "  '.'],\n",
       " ['check',\n",
       "  'out',\n",
       "  'the',\n",
       "  'course',\n",
       "  'on',\n",
       "  'Github',\n",
       "  ':',\n",
       "  'https://github.com/bpben/nlp_lessons']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_full(text_data, stop_words=True, alpha_only=False, entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "A very basic way to use a sanitized list of tokens is to do a word count.  This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "tokenized = [simple_tokenizer(doc) for doc in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of counts: [Counter({'i': 1, 'taking': 1, 'a': 1, 'course': 1, 'at': 1, 'harvard': 1}), Counter({'i': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1}), Counter({'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1}), Counter({'check': 1, 'out': 1, 'the': 1, 'course': 1, 'on': 1, 'github': 1})]\n",
      "[Counter({'i': 1, 'taking': 1, 'a': 1, 'course': 1, 'at': 1, 'harvard': 1}), Counter({'i': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1}), Counter({'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1}), Counter({'check': 1, 'out': 1, 'the': 1, 'course': 1, 'on': 1, 'github': 1})]\n",
      "\n",
      "Combined count: Counter({'i': 2, 'course': 2, 'taking': 1, 'a': 1, 'at': 1, 'harvard': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1, 'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1, 'check': 1, 'out': 1, 'the': 1, 'on': 1, 'github': 1})\n"
     ]
    }
   ],
   "source": [
    "# base python: create an make use of a Counter object\n",
    "counts = [Counter(d) for d in tokenized]\n",
    "print('List of counts:', counts)\n",
    "# sum together all counts\n",
    "all_counts = Counter()\n",
    "for d in tokenized:\n",
    "    all_counts += Counter(d)\n",
    "print(counts)\n",
    "print('\\nCombined count:', all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "# result is the same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sentiment analysis with word counts\n",
    "Imagine you are a hot dog restaurant owner and you want to analyze a corpus of reviews from diners to see whether people generally think your hot dogs are \"good\" or \"bad\".  Specifically, you're going to count up the number of times the word \"good\" and word \"bad\" appears.  Depending on how you process the text, you will arrive at different conclusions.  Try a couple ways to see what I mean.\n",
    "\n",
    "You might also want to think about whether all the reviews are relevant.  Those sorts of choices may also affect your results.  Is there an automatic way you can remove non-relevant reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reviews = ['These hot dogs are really good.',\n",
    "          'These hot dogs are really bad.',\n",
    "          'Good hot dogs!',\n",
    "          'The hot dogs pair well with a Good Humor bar.',\n",
    "          \"I didn't eat anything, I felt bad.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as expected.  Why don't we try this on Assignment 1's dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = '../data/movie_reviews_subset.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15365),\n",
       " ('a', 7548),\n",
       " ('and', 6978),\n",
       " ('to', 6780),\n",
       " ('of', 6402),\n",
       " ('is', 4952),\n",
       " ('it', 4354),\n",
       " ('i', 4248),\n",
       " ('in', 4203),\n",
       " ('this', 3837)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this on negative reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "neg_vectors = cv.fit_transform(neg).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), neg_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 17182),\n",
       " ('and', 8905),\n",
       " ('a', 8133),\n",
       " ('of', 7942),\n",
       " ('to', 6658),\n",
       " ('is', 5793),\n",
       " ('in', 5093),\n",
       " ('it', 4553),\n",
       " ('i', 3943),\n",
       " ('that', 3521)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do it for positive reviews\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "pos_vectors = cv.fit_transform(pos).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "word_count = dict(zip(cv.get_feature_names(), pos_vectors.sum(axis=0)))\n",
    "# get the top 10 words\n",
    "sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words aren't particularly informative about the content.  Sklearn's CountVectorizer has some additional options that may lead to somewhat more informative frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mainpy3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'nt'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 2297), ('film', 1797), ('like', 1085), ('just', 984), ('bad', 668), ('did', 637), ('good', 632), ('really', 620), ('time', 560), ('does', 521)]\n",
      "[('film', 2122), ('movie', 1904), ('like', 891), ('good', 811), ('just', 713), ('great', 654), ('story', 625), ('time', 604), ('really', 539), ('does', 508)]\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    cv = CountVectorizer(tokenizer=simple_tokenizer, min_df=0.01, max_df=0.9,\n",
    "                        stop_words='english')\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    # get_feature_names gets the vocabulary of the vectorizer in order\n",
    "    word_count = dict(zip(cv.get_feature_names(), vectors.sum(axis=0)))\n",
    "    # get the top 10 words\n",
    "    print(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it seems like we'd have to tweak these thresholds a lot and carefully choose our stop words.  Is there a more standard way to extract the most informative words from documents?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "See the slides for more information on this.  In this section we'll show how TF-IDF is essentially just a weighting of the count vectors.  We'll then use sklearn's built-in TfidfVectorizer on our sentiment corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  great  movie  the  was\n",
       "0    0     1      0      1    1    1\n",
       "1    1     0      0      1    1    1\n",
       "2    0     0      1      1    1    1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The movie was good',\n",
    "        'The movie was bad',\n",
    "        'The movie was great']\n",
    "\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "vecs = cv.fit_transform(docs).toarray()\n",
    "# we'll use pandas DF for easier display\n",
    "pd.DataFrame(vecs, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that `vecs` contains the term frequencies.  If we use sklearn's `TfidfVectorizer`, it will calculate those term counts and then multiply them by the Inverse Document Frequency (IDF).\n",
    "\n",
    "The formula sklearn uses is a bit different from the textbook:\n",
    "\n",
    "$$log(\\frac{N+1}{df(t)+1}) + 1$$\n",
    "\n",
    "Where $N$ is the number of documents.  It also normalizes this value to account for different size vectors (see slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>movie</th>\n",
       "      <th>the</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.69903</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "      <td>0.412859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad     good    great     movie       the       was\n",
       "0  0.00000  0.69903  0.00000  0.412859  0.412859  0.412859\n",
       "1  0.69903  0.00000  0.00000  0.412859  0.412859  0.412859\n",
       "2  0.00000  0.00000  0.69903  0.412859  0.412859  0.412859"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer)\n",
    "# we'll use pandas DF for easier display\n",
    "tfidf_vecs = tfidf.fit_transform(docs).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_vecs, columns=tfidf.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the discriminative words (i.e. bad, good, great) have higher weight than the non-discriminative words.  \n",
    "\n",
    "We see this at the document level, but is there a way we could get some kind of aggregate measure of discriminative words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the top 3 discriminative words\n",
    "Use the dataset above to try and identify the words that, across the corpus, are particularly representative of content.\n",
    "\n",
    "Hint: Think about what a weight of zero versus weight of non-zero means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_tfidf_words(tfidf_df):\n",
    "    return(tfidf_df[tfidf_df>0].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad      0.699030\n",
       "good     0.699030\n",
       "great    0.699030\n",
       "movie    0.412859\n",
       "the      0.412859\n",
       "was      0.412859\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_words(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that on our movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children    0.151333\n",
      "monster     0.152135\n",
      "dr          0.153720\n",
      "cute        0.156171\n",
      "island      0.157478\n",
      "space       0.165138\n",
      "theater     0.165205\n",
      "nt          0.167994\n",
      "game        0.189159\n",
      "the         0.267655\n",
      "dtype: float64\n",
      "and         0.153424\n",
      "oscar       0.154922\n",
      "dance       0.155299\n",
      "edge        0.159386\n",
      "rock        0.162227\n",
      "western     0.165590\n",
      "ryan        0.167666\n",
      "japanese    0.172789\n",
      "dr          0.177205\n",
      "the         0.274663\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for corpus in [neg, pos]:\n",
    "    # adding in a minimum document frequency, so words need to occur at least somewhat often\n",
    "    tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, min_df=0.02)\n",
    "    vectors = tfidf.fit_transform(corpus).toarray()\n",
    "    tfidf_df = pd.DataFrame(vectors, columns=tfidf.get_feature_names())\n",
    "    # get representative words\n",
    "    tfidf_word_count = top_tfidf_words(tfidf_df)\n",
    "    # get the top 10 words\n",
    "    print(tfidf_word_count.sort_values().iloc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are somewhat useful aggregate measures.  But most of the information in TF-IDF is document-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Topic models: Non-negative Matrix Factorization and Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mainpy3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'nt'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/opt/anaconda3/envs/mainpy3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'nt'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# in this case, excluding standard english stop words\n",
    "# creating a combined review dataset\n",
    "all_reviews = neg+pos\n",
    "tfidf = TfidfVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "tfidf_vecs = tfidf.fit_transform(all_reviews)\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer, stop_words='english')\n",
    "count_vecs = cv.fit_transform(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# choose the number of components (topics)\n",
    "n_components = 10\n",
    "# basic configuration\n",
    "nmf = NMF(n_components=n_components)\n",
    "# NMF requires tfidf, not word counts\n",
    "# same syntax as vectorizer\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "# LDA uses word counts\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NMF and LDA provide a components matrix which corresponds to the loading of each word on each topic.  Higher values means the word is more relevant to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.14459699e-03 0.00000000e+00 3.70323998e-03 ... 1.90700493e-05\n",
      "  6.86192149e-04 6.86192149e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.29703942e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.42693943e-04\n",
      "  2.33893836e-03 2.33893836e-03]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.13925918e-03 ... 6.81188262e-05\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 4.35862023e-05 0.00000000e+00 ... 1.53815885e-04\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating performance, both methods use different ways to quantify the loss from using the topic model versus the actual data.  (In the matrix formulation, $UV$ rather than $X$).  For NMF, it's reconstruction error, which is more directly the difference between the matrix decomposition and the actual data.  For LDA, it uses [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound), which is a too complicated to explain here.  In both, higher values means worse performance.  They can't be compared to one another, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.814189927095335 7011.152878238849\n"
     ]
    }
   ],
   "source": [
    "print(nmf.reconstruction_err_, lda.bound_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film films good seen does\n",
      "Topic 1:\n",
      "movie watch good movies time\n",
      "Topic 2:\n",
      "man life story family young\n",
      "Topic 3:\n",
      "series episode episodes tv season\n",
      "Topic 4:\n",
      "br money music audience thing\n",
      "Topic 5:\n",
      "great best good action love\n",
      "Topic 6:\n",
      "did like just people really\n",
      "Topic 7:\n",
      "book read story novel character\n",
      "Topic 8:\n",
      "bad acting good worst movies\n",
      "Topic 9:\n",
      "horror effects special budget gore\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "movie film like great good\n",
      "Topic 1:\n",
      "film movie story like good\n",
      "Topic 2:\n",
      "film movie good just like\n",
      "Topic 3:\n",
      "film movie like just make\n",
      "Topic 4:\n",
      "movie film like just good\n",
      "Topic 5:\n",
      "film like way just characters\n",
      "Topic 6:\n",
      "like just time does movie\n",
      "Topic 7:\n",
      "movie film like just people\n",
      "Topic 8:\n",
      "movie like film just good\n",
      "Topic 9:\n",
      "movie film like story good\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF seems to have come up with some reasonable topics, but LDA doesn't seem to work particularly well here.  It may make sense to try some additional token processing and see how that affects what we get out of the topic modelling process.\n",
    "\n",
    "### Exercise: Tokenization decisions and topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized = tokenize_full(all_reviews, entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if passing a list of tokens to a vectorizer, you can use the following syntax\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vecs = tfidf.fit_transform(tokenized)\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "count_vecs = cv.fit_transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "nmf = NMF(n_components=n_components)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)\n",
    "lda = LatentDirichletAllocation(n_components=n_components)\n",
    "lda_vecs = lda.fit_transform(count_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film see watch good time\n",
      "Topic 1:\n",
      "movie watch see think time\n",
      "Topic 2:\n",
      "love life story family young\n",
      "Topic 3:\n",
      "like scene go look people\n",
      "Topic 4:\n",
      "series episode watch season funny\n",
      "Topic 5:\n",
      "br music money play audience\n",
      "Topic 6:\n",
      "great good actor cast acting\n",
      "Topic 7:\n",
      "book read story character novel\n",
      "Topic 8:\n",
      "bad acting see terrible waste\n",
      "Topic 9:\n",
      "game video play level like\n"
     ]
    }
   ],
   "source": [
    "display_components(nmf, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "Barney France take solve find\n",
      "Topic 1:\n",
      "film movie good br scene\n",
      "Topic 2:\n",
      "movie like film time scene\n",
      "Topic 3:\n",
      "movie like bad good film\n",
      "Topic 4:\n",
      "movie film good like time\n",
      "Topic 5:\n",
      "movie film like good character\n",
      "Topic 6:\n",
      "film br character scene come\n",
      "Topic 7:\n",
      "film movie good like story\n",
      "Topic 8:\n",
      "film like time scene movie\n",
      "Topic 9:\n",
      "movie film watch good like\n"
     ]
    }
   ],
   "source": [
    "display_components(lda, cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
